#### Elasticsearch定义

+ Elasticsearch是一个开源的**高扩展的分布式全文检索引擎**，它可以近乎实时的存储、检索数据；本身扩展性很好，可以扩展到上百台服务器，处理PB级别的数据。 
+ Elasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单。

#### ES核心概念

##### Cluster：集群。

+ 集群包含多个节点，每个节点属于哪个集群是通过配置来决定的，对于中小型应用来说，刚开始一个集群就一个节点很正常

##### Node：节点

+ 形成集群的每个服务器称为节点。

##### Shard：分片

+ 当有大量的文档时，由于内存的限制、磁盘处理能力不足、无法足够快的响应客户端的请求等，一个节点可能不够。这种情况下，数据可以分为较小的分片。每个分片放到不同的服务器上。 

+ 当你查询的索引分布在多个分片上时，ES会把查询发送给每个相关的分片，并将结果组合在一起，而应用程序并不知道分片的存在。即：这个过程对用户来说是透明的。

##### Repliacs：副本

+ 为提高查询吞吐量或实现高可用性，可以使用分片副本。 副本是一个分片的精确复制，每个分片可以有零个或多个副本。ES中可以有许多相同的分片，其中之一被选择更改索引操作，这种特殊的分片称为主分片。 当主分片丢失时，如：该分片所在的数据不可用时，集群将副本提升为新的主分片。

##### 全文检索

+ 全文检索就是对一篇文章进行索引，可以根据关键字搜索，类似于mysql里的like语句。 全文索引就是把内容根据词的意义进行分词，然后分别创建索引，例如”你们的激情是因为什么事情来的” 可能会被分词成：“你们“，”激情“，“什么事情“，”来“ 等token，这样当你搜索“你们” 或者 “激情” 都会把这句搜出来。

##### 近实时

+ 近实时，两个意思，从写入数据到数据可以被搜索到有一个小延迟（大概1秒）；基于es执行搜索和分析可以达到秒级。

##### Index（索引-数据库）

+ 索引包含一堆有相似结构的文档数据，比如可以有一个客户索引，商品索引，订单索引，索引有一个名称。一个index包含很多document，一个index就代表了一类相同的document。比如说建立一个goods index，商品索引，里面可能就存放了所有的商品数据，所有的商品document。

##### Type（类型-表）

+ 在 7.X 版本中，直接去除了 type 的概念，就是说 index 不再会有 type

+ 每个索引里都可以有一个或多个type，type是index中的一个逻辑数据分类，一个type下的document，都有相同的field，比如博客系统，有一个索引，可以定义用户数据type，博客数据type，评论数据type。

##### Document（文档-行）
+ 文档是es中的最小数据单元，一个document可以是一条客户数据，一条商品分类数据，一条订单数据，通常用JSON数据结构表示，每个index下的type中，都可以去存储多个document。

##### Field（字段-列）
+ Field是Elasticsearch的最小单位。一个document里面有多个field，每个field就是一个数据字段

##### mapping（映射-约束）

+ 数据如何存放到索引对象上，需要有一个映射配置，包括：数据类型、是否存储、是否分词等。

+ 映射就是相当于mysql的表结构，但是不一样的是es支持手动创建表结构，也支持自动创建表结构（就是在存放数据的时候自动创建自动分析数据类型）

  在创建Mapping 时指定就可以。Mapping用来定义Document中每个字段的类型，即所使用的 analyzer、是否索引等属性，非常关键等。创建Mapping 的代码示例如下：

  ```
  {
  	"mappings": {
  		"properties": {
  			"goods_name": {
  				"analyzer": "ik_max_word",
  				"fields": {
  					"keyword": {
  						"ignore_above": 256,
  						"type": "keyword"
  					}
  				},
  				"type": "text"
  			},
  			"pro_price": {
  				"type": "float"
  			},
  			"description": {
  				"analyzer": "ik_max_word",
  				"fields": {
  					"keyword": {
  						"ignore_above": 521,
  						"type": "keyword"
  					}
  				},
  				"type": "text"
  			}
  		}
  	},
  	"settings": {
  		"analysis": {
  			"analyzer": {
  				"ik": {
  					"tokenizer": "ik_max_word"
  				}
  			}
  		}
  	}
  }
  ```





























